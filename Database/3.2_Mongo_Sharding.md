# Sharding

## **Example**

### MongoDB Sharding Command

```javascript
// Enable sharding on database
sh.enableSharding("mydb")

// Choose shard key for collection
sh.shardCollection("mydb.users", { userId: 1 })
```

**Data Distribution:**

```
Shard 1: { userId: 1-1000 }
Shard 2: { userId: 1001-2000 }
Shard 3: { userId: 2001-3000 }
```

---

## **Sharding in Other Systems**

* **MySQL:** No native sharding â€” must be done at app level or with tools like Vitess, ProxySQL.
* **Redis Cluster:** Uses **hash slots** (0â€“16383) to distribute keys to nodes.
* **Elasticsearch:** Built-in sharding for index segments.

---

### **Quick Analogy**

* **Sharding** â†’ Splitting your contact list alphabetically (Aâ€“E, Fâ€“J, etc.).
* **Sharded Cluster** â†’ The **whole system** where each person (server) holds part of the contact list, and a **directory** (config server) knows who has which names.

---

## **ğŸ“Œ 1. What is Sharding?**

Sharding = **Horizontal partitioning** â†’ large datasets are split into **chunks** and stored across **multiple servers** (**shards**).
Goals:

* Scale storage beyond a single machine
* Distribute load
* Keep performance stable as data grows

---

## **ğŸ›  2. What is a Sharded Cluster in MongoDB?**

A **sharded cluster** is MongoDBâ€™s architecture for scaling horizontally.

**Main components:**

* **Shards** â†’ Store actual data.
* **Config servers** â†’ Keep metadata (chunk â†’ shard mapping).
* **mongos router** â†’ Receives queries and routes them to the right shard(s).

```
[Client] â†’ [mongos router] â†’ [Shard 1 | Shard 2 | Shard 3]
                     â†‘
               [Config Servers]
```

---

## **ğŸ· 3. Types of Sharded Clusters**

| Type                                     | Description                                                                              | Example                                                               | Pros                                        | Cons                                                    |
| ---------------------------------------- | ---------------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------------- |
| **Standalone Shards**                    | Each shard is a single MongoDB instance (no replication).                                | Shard 1 stores users Aâ€“M, Shard 2 stores Nâ€“Z.                         | Simple, low resource usage                  | No high availability â€” if one shard fails, data is gone |
| **Replica Set Shards** âœ… *(recommended)* | Each shard is a **replica set** â€” multiple nodes with the same data for fault tolerance. | Shard 1 (Primary + 2 Secondaries), Shard 2 (Primary + 2 Secondaries). | High availability, fault tolerance, scaling | More complex setup, higher resource cost                |

ğŸ’¡ In production, **MongoDB sharded clusters almost always use replica set shards**.

---

## **ğŸ”‘ 3. Types of Sharding in MongoDB**

### **a) Range-Based Sharding**

* Documents are split into **contiguous ranges** based on shard key values.
* Example:

  ```
  Shard 1 â†’ user_id 1â€“1000
  Shard 2 â†’ user_id 1001â€“2000
  ```

* **Pros:** Fast for range queries.
* **Cons:** Sequential keys (like timestamps) can cause hotspots.

---

### **b) Hash-Based Sharding**

* A **hash function** is applied to the shard key value before deciding chunk location.
* Example:

  ```
  hash(user_id) â†’ chunk â†’ shard
  ```

* **Pros:** Even distribution, avoids hotspots.
* **Cons:** Range queries become scatter-gather.

---

### **c) Zoned (Tag-Aware) Sharding**

* Specific **ranges** of shard key values are assigned to certain shards (**zones**).
* Example:

  ```
  Zone "US-East" â†’ Shard 1
  Zone "US-West" â†’ Shard 2
  ```

* **Pros:** Geo-partitioning & compliance.
* **Cons:** Requires careful planning.

---

## **ğŸ“ 5. How MongoDB Knows Which Shard to Check**

1. Query arrives at **mongos**.
2. If query contains **shard key** â†’ mongos uses **config servers** to find the **chunk location** â†’ forwards to the correct shard(s). âœ… (fast)
3. If query has **no shard key** â†’ mongos must **scatter-gather** (query all shards and merge results). âŒ (slower)
4. The **balancer** process moves chunks to keep shards evenly loaded.

---

## **ğŸ“Œ 6. Full Example**

Letâ€™s say:

* Collection: `orders`
* Shard key: `{ user_id: 1 }`
* Cluster type: **Sharded cluster with replica set shards**
* Sharding type: **Range-based**

**Chunk distribution:**

```
Chunk 1 â†’ Shard 1 (user_id 1â€“1000)
Chunk 2 â†’ Shard 2 (user_id 1001â€“2000)
```

**Query 1:**

```js
db.orders.find({ user_id: 450 })
```

* Contains shard key â†’ mongos â†’ chunk lookup â†’ only Shard 1 queried âœ…

**Query 2:**

```js
db.orders.find({ amount: { $gt: 100 } })
```

* No shard key â†’ mongos queries **all shards** â†’ merges results âŒ

---

## **ğŸ“Š Summary Table**

| Concept                | What It Means                                   | Example                               |
| ---------------------- | ----------------------------------------------- | ------------------------------------- |
| **Sharded Cluster**    | MongoDBâ€™s multi-server architecture for scaling | Orders split across Shard 1 & Shard 2 |
| **Standalone Shards**  | Single MongoDB instance per shard               | Shard 1 â†’ Aâ€“M, Shard 2 â†’ Nâ€“Z          |
| **Replica Set Shards** | Each shard is a replica set                     | Shard 1 (Primary + 2 Secondaries)     |
| **Range Sharding**     | Continuous ranges of shard key values           | 1â€“1000 in Shard 1                     |
| **Hash Sharding**      | Distribute by hash of shard key                 | hash(user\_id) decides shard          |
| **Zoned Sharding**     | Map shard key ranges to zones                   | US-East â†’ Shard 1                     |

---
## Diagram

![alt text](/Assets/shards.png)