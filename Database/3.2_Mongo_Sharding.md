# Sharding

## **Example**

### MongoDB Sharding Command

```javascript
// Enable sharding on database
sh.enableSharding("mydb")

// Choose shard key for collection
sh.shardCollection("mydb.users", { userId: 1 })
```

**Data Distribution:**

```
Shard 1: { userId: 1-1000 }
Shard 2: { userId: 1001-2000 }
Shard 3: { userId: 2001-3000 }
```

---

## **Sharding in Other Systems**

* **MySQL:** No native sharding — must be done at app level or with tools like Vitess, ProxySQL.
* **Redis Cluster:** Uses **hash slots** (0–16383) to distribute keys to nodes.
* **Elasticsearch:** Built-in sharding for index segments.

---

### **Quick Analogy**

* **Sharding** → Splitting your contact list alphabetically (A–E, F–J, etc.).
* **Sharded Cluster** → The **whole system** where each person (server) holds part of the contact list, and a **directory** (config server) knows who has which names.

---

## **📌 1. What is Sharding?**

Sharding = **Horizontal partitioning** → large datasets are split into **chunks** and stored across **multiple servers** (**shards**).
Goals:

* Scale storage beyond a single machine
* Distribute load
* Keep performance stable as data grows

---

## **🛠 2. What is a Sharded Cluster in MongoDB?**

A **sharded cluster** is MongoDB’s architecture for scaling horizontally.

**Main components:**

* **Shards** → Store actual data.
* **Config servers** → Keep metadata (chunk → shard mapping).
* **mongos router** → Receives queries and routes them to the right shard(s).

```
[Client] → [mongos router] → [Shard 1 | Shard 2 | Shard 3]
                     ↑
               [Config Servers]
```

---

## **🏷 3. Types of Sharded Clusters**

| Type                                     | Description                                                                              | Example                                                               | Pros                                        | Cons                                                    |
| ---------------------------------------- | ---------------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------------- |
| **Standalone Shards**                    | Each shard is a single MongoDB instance (no replication).                                | Shard 1 stores users A–M, Shard 2 stores N–Z.                         | Simple, low resource usage                  | No high availability — if one shard fails, data is gone |
| **Replica Set Shards** ✅ *(recommended)* | Each shard is a **replica set** — multiple nodes with the same data for fault tolerance. | Shard 1 (Primary + 2 Secondaries), Shard 2 (Primary + 2 Secondaries). | High availability, fault tolerance, scaling | More complex setup, higher resource cost                |

💡 In production, **MongoDB sharded clusters almost always use replica set shards**.

---

## **🔑 3. Types of Sharding in MongoDB**

### **a) Range-Based Sharding**

* Documents are split into **contiguous ranges** based on shard key values.
* Example:

  ```
  Shard 1 → user_id 1–1000
  Shard 2 → user_id 1001–2000
  ```

* **Pros:** Fast for range queries.
* **Cons:** Sequential keys (like timestamps) can cause hotspots.

---

### **b) Hash-Based Sharding**

* A **hash function** is applied to the shard key value before deciding chunk location.
* Example:

  ```
  hash(user_id) → chunk → shard
  ```

* **Pros:** Even distribution, avoids hotspots.
* **Cons:** Range queries become scatter-gather.

---

### **c) Zoned (Tag-Aware) Sharding**

* Specific **ranges** of shard key values are assigned to certain shards (**zones**).
* Example:

  ```
  Zone "US-East" → Shard 1
  Zone "US-West" → Shard 2
  ```

* **Pros:** Geo-partitioning & compliance.
* **Cons:** Requires careful planning.

---

## **📍 5. How MongoDB Knows Which Shard to Check**

1. Query arrives at **mongos**.
2. If query contains **shard key** → mongos uses **config servers** to find the **chunk location** → forwards to the correct shard(s). ✅ (fast)
3. If query has **no shard key** → mongos must **scatter-gather** (query all shards and merge results). ❌ (slower)
4. The **balancer** process moves chunks to keep shards evenly loaded.

---

## **📌 6. Full Example**

Let’s say:

* Collection: `orders`
* Shard key: `{ user_id: 1 }`
* Cluster type: **Sharded cluster with replica set shards**
* Sharding type: **Range-based**

**Chunk distribution:**

```
Chunk 1 → Shard 1 (user_id 1–1000)
Chunk 2 → Shard 2 (user_id 1001–2000)
```

**Query 1:**

```js
db.orders.find({ user_id: 450 })
```

* Contains shard key → mongos → chunk lookup → only Shard 1 queried ✅

**Query 2:**

```js
db.orders.find({ amount: { $gt: 100 } })
```

* No shard key → mongos queries **all shards** → merges results ❌

---

## **📊 Summary Table**

| Concept                | What It Means                                   | Example                               |
| ---------------------- | ----------------------------------------------- | ------------------------------------- |
| **Sharded Cluster**    | MongoDB’s multi-server architecture for scaling | Orders split across Shard 1 & Shard 2 |
| **Standalone Shards**  | Single MongoDB instance per shard               | Shard 1 → A–M, Shard 2 → N–Z          |
| **Replica Set Shards** | Each shard is a replica set                     | Shard 1 (Primary + 2 Secondaries)     |
| **Range Sharding**     | Continuous ranges of shard key values           | 1–1000 in Shard 1                     |
| **Hash Sharding**      | Distribute by hash of shard key                 | hash(user\_id) decides shard          |
| **Zoned Sharding**     | Map shard key ranges to zones                   | US-East → Shard 1                     |

---
## Diagram

![alt text](/Assets/shards.png)